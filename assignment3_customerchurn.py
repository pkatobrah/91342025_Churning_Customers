# -*- coding: utf-8 -*-
"""Assignment3_CustomerChurn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rYYK_53zP4gAjtoexTmUi7hQkpv2goGv
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
from mlxtend.plotting import plot_decision_regions
import missingno as msno
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
#from sklearn.neighbors import KNeighbors_Classifier
from sklearn.metrics import confusion_matrix
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import GridSearchCV
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

!pip install keras-tuner
import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras.layers import Input, Dense
from keras import models
from keras import Model
from tensorflow.keras import layers
import kerastuner as kt
from kerastuner.tuners import RandomSearch
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
!pip install scikit-plot
import scikitplot as skplt

from google.colab import drive
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
drive.mount("/content/drive")

data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CustomerChurn_dataset.csv')

data.info()

data = data.drop('customerID', axis =1)

data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce', downcast = 'float')

data.isnull().sum()

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
data[['TotalCharges']] = imputer.fit_transform(data[['TotalCharges']])

data.info()

new_data = data.copy()

from sklearn.preprocessing import LabelEncoder
categorical_columns = data.select_dtypes(include=['object']).columns
label_encoder = LabelEncoder()

for col in categorical_columns:
    data[col] = label_encoder.fit_transform(data[col])

data

correlation_matrix = data.corr()
important_features = correlation_matrix["Churn"].abs().sort_values(ascending=False)
selected_features = important_features[1:7].index

selected_features

# Visualize distribution of 'TotalCharges' for churned and non-churned customers
sns.boxplot(x='Churn', y='TotalCharges', data=new_data)
plt.show()
print("It shows that when the total charges are lower, the churn is higher")
print()
print()

# Visualize the relationship between 'Contract' and churn
sns.countplot(x='Contract', hue='Churn', data=new_data)
plt.show()
print("It shows that month to month contracts have the most churns")
print()
print()

# Visualize the relationship between 'tenure' and churn
sns.boxplot(x='Churn', y='tenure', data=new_data)
plt.show()
print("It shows that shorter tenures have more churns")
print()
print()

# Visualize the relationship between 'OnlineSecurity' and churn
sns.countplot(x='OnlineSecurity', hue='Churn', data=new_data)
plt.show()
print("It shows that no online security has the most churns")
print()
print()

# Visualize the relationship between 'TechSupport' and churn
sns.countplot(x='TechSupport', hue='Churn', data=new_data)
plt.show()
print("It shows that no tech support has the most churns")
print()
print()

# Visualize the relationship between 'OnlineBackup' and churn
sns.countplot(x='OnlineBackup', hue='Churn', data=new_data)
plt.show()
print("It shows that when there is no online backup there are more churns")
print()
print()

y = data['Churn']
X = data[selected_features]

selected_features

scaler = StandardScaler()

X_scaled = scaler.fit_transform(X.copy())


X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

data = pd.concat([X_scaled_df, y], axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import keras
from keras.models import Model
from keras.layers import Input, Dense
from keras.optimizers import Adam
from keras.utils import to_categorical

# Keras Functional API model

#Input layer
input_layer = Input(shape=(X_train.shape[1],))

#Hidden layer
hidden_layer_1 = Dense(32, activation='relu')(input_layer)
hidden_layer_2 = Dense(24, activation='relu')(hidden_layer_1)
hidden_layer_3 = Dense(12, activation='relu')(hidden_layer_2)


#)Outputvlayer
output_layer = Dense(1, activation='sigmoid')(hidden_layer_3)

model = Model(inputs=input_layer, outputs=output_layer)

model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=120, batch_size=32, validation_data=(X_test, y_test))

_, accuracy = model.evaluate(X_train, y_train)
accuracy*100

loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy*100:.4f}')

!pip install keras tuner

from tensorflow import keras
from tensorflow.keras import layers

def build_model(hp):
    model = keras.Sequential()
    model.add(layers.InputLayer(input_shape=(X_train.shape[1],)))

    # Tune the number of hidden layers and units
    for i in range(hp.Int('num_hidden_layers', min_value=1, max_value=4)):
        units = hp.Int(f'units_{i}', min_value=32, max_value=96, step=32)
        activation = hp.Choice(f'activation_{i}', values=['relu', 'tanh'])
        model.add(layers.Dense(units=units, activation=activation))

    model.add(layers.Dense(1, activation='sigmoid'))

    # Tune the learning rate
    learning_rate = hp.Float("lr", min_value=1e-4, max_value=1e-2, sampling="log")
    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)

    model.compile(
        optimizer=optimizer,
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    return model

build_model(keras_tuner.HyperParameters())

tuner = keras_tuner.Hyperband(
  hypermodel=build_model,
  objective='val_accuracy',
  max_epochs=100,
  factor=3,
  directory='tuning_dir',
  project_name='samples')

tuner.search(X_train, y_train, epochs=30 ,validation_data=(X_test, y_test))

best_model = tuner.get_best_models(num_models=2)[0]

best_model.summary()

y_pred = best_model.predict(X_test)
y_pred_binary = (y_pred > 0.5).astype(int)

accuracy = accuracy_score(y_test, y_pred_binary)
auc_score = roc_auc_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print(f"AUC Score: {auc_score}")

best_model.save("best_model.h5")

import pickle
with open('standard_scaler.pkl', 'wb') as file:
    pickle.dump(scaler, file)